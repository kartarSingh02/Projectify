# -*- coding: utf-8 -*-
"""Copy of ML_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ymybWw7EFczmttwEdcPk1JGe8v2kP1-C
"""

# !pip install spacy
# !python -m spacy download en_core_web_sm

# pip install python-pptx

# pip install PyPDF2

import os
import json
import pandas as pd
import PyPDF2
from pptx import Presentation
import re

path = r'/content/data'

os.chdir(path)



def read_pdf_file(file_path):
    
    fileObj = open(file_path, 'rb')
  
    # creating a pdf reader object
    pdfReader = PyPDF2.PdfReader(fileObj)

    # printing number of pages in pdf file
    # print(len(pdfReader.pages))

    # creating a page object
    pageObj = pdfReader.pages[0]

    # extracting text from page
    # print(type(pageObj.extract_text()))

    data_in_pdf = ''

    for page in pdfReader.pages:
        data_in_pdf += page.extract_text()

    return data_in_pdf





# dictionary to hold data before putting into df
data_dict = {}
data_dict['file_name'] = []
data_dict['file_path'] = []
data_dict['file_extension'] = []
data_dict['file_data'] = []

# iterate through all file
for file in os.listdir():
    
    if file == '.ipynb_checkpoints':
        continue
    
    file_path = f"{path}/{file}"
    
    data_dict['file_name'].append(((file_path.split('/')[-1]).split('.')[0]).replace('-','_'))
    data_dict['file_path'].append(file_path)
    data_dict['file_extension'].append(file_path.split('.')[-1])
    
    final_data = " "
    
    
    if file.lower().endswith('.pdf'):
        final_data = read_pdf_file(file_path) 
    else:
        print(file)
        raise Exception('UNSUPPORTED FILE TYPE')


    data_dict['file_data'].append(final_data)

df=pd.DataFrame.from_dict(data_dict)
# pd.set_option('display.max_colwidth',None)  # use to full text or data

# display(df)

import spacy
from spacy.matcher import Matcher

# load english language model
nlp = spacy.load('en_core_web_sm')

def read_pdf(path):
  
  # !wget https://drive.google.com/file/d/1_AAXjZbP2ntC3tMPaZ-HkCIm9rDwKMat/view?usp=sharing -O work_pdf2.pdf

  # fileObj = open('/content/work_pdf2.pdf', 'rb')

  fileObj = open(path, 'rb')
  pdfReader = PyPDF2.PdfReader(fileObj)

  text = ''

  for page in pdfReader.pages:
    text += page.extract_text()

  # print(text)  
  return text

# def clean(text):
    
# # adding space before and after '\n'
#     text = re.sub('\n', ' \n ', str(text))
    
# # adding before and space after URL
#     text = re.sub('URL', ' URL ', str(text))

# # adding space before and after 'Project'
#     text = re.sub('Project', ' Project ', str(text))
#     text = re.sub('PROJECT', ' PROJECT ', str(text))
    
# # replacing '\xa0' with ' '
#     text = re.sub('\xa0', ' ', str(text))
    
# # replace 'Go-Live' with '\n Go-Live'
#     text = re.sub('Go-Live', '\n Go-Live', str(text))
    
# # replace 'Slide' with ' Slide'
#     text = re.sub('Slide', ' Slide', str(text))
    
# # removing •
#     text = re.sub('•', ' ', str(text))
    
# # removing §
#     text = re.sub('§', ' ', str(text))
    
#     return text
def clean(text):
    
# adding space before and after '\n'
    text = re.sub('\n', ' \n ', str(text))

    text = re.sub('Slide', ' Slide ', str(text))
    
# adding before and space after URL
    text = re.sub('URL', ' URL ', str(text))

# adding space before and after 'Project'
    text = re.sub('Project', ' Project ', str(text))
    text = re.sub('PROJECT', ' PROJECT ', str(text))
    
# replacing '\xa0' with ' '
    text = re.sub('\xa0', ' ', str(text))
    
# replace 'Go-Live' with '\n Go-Live'
    text = re.sub('Go-Live', '\n Go-Live', str(text))
    
# replace 'Slide' with ' Slide'
    text = re.sub('Slide', ' Slide', str(text))
    
# removing •
    text = re.sub('•', ' ', str(text))
    
# removing §
    text = re.sub('§', ' ', str(text))
    
#     # removing apostrophes
#     text = re.sub("'s",'',str(text))

#     # removing hyphens
#     text = re.sub("-",' ',str(text))
#     text = re.sub("— ",'',str(text))

#     # removing quotation marks
#     text = re.sub('\"','',str(text))

#     # removing any reference to outside text
#     text = re.sub("[\(\[].*?[\)\]]", "", str(text))
    
    return text

def check(projects):
  res_list=[]
  for i in projects:
    i=i.replace('(','')
    x=i.strip()
    x= re.sub(r'\s+', ' ', x)
    res_list.append(x)


  unique_list = list(set(res_list))
  return unique_list




def find_project_name(text,path):

    pattern = r'Project  \d+ : (.+)'
    # pattern = r'(?<=:)\s*(\b\w+\b(\s+\b\w+\b)*)\s*(?!\')'
    
    projects = re.findall(pattern, text)

    pattern = r'Projects: (.+)'
    projects = re.findall(pattern, text)
    pattern = r'Project  Name : (.+)'
        
    projects = re.findall(pattern, text)
    s="project"
    

   
    
    if len(projects) == 0:
      value=find_client_name(path)
      return [(value+" "+s)]


        
        
    

    
    return check(projects)

def find_client_name(text):
  # /content/data/Activision Case Study.pdf

    
    new_text=text.split('.')[0]
    new_text = new_text.replace('_PPT', ' ').replace('_Case_Study',' ').replace('Case Study',' ').replace('PPT',' ').replace('-',' ').replace('_',' ').replace('projects','').replace('project','')
    # print(new_text)
    # new_text=new_text.trim()
    new_text=re.sub(r'\d', '', new_text)
    new_text=new_text.strip()
    new_text = re.sub(r'\s+', ' ', new_text)

    return new_text

    # # pattern = r'^(\w+&\w+)'
    # match = re.search(r'\w+&\w+', new_text)

    # if match:
        
    #     match_str = match.group()
    #     return match_str
    
    # else:
        
    #     pattern = r'^(\w+)'
    #     clients = re.search(pattern, new_text).group()
    #     return clients

# def find_project_desc(text):

#   pattern = r'Project  Description – (.+)| Slide \d+ (.+)'
  
#   projects = re.findall(pattern, text)

 
  

#   final_output = []

#   for project in projects:
#       final_output.append("".join(project))
        
#   for i in range(len(final_output)):
#       final_output[i] = re.sub('  ', ' ', final_output[i])
#       final_output[i] = final_output[i].strip()
#       final_output[i] += '...'


#   return projects

def find_project_desc(text,path):

  pattern = r'Project  Description – (.+)| Slide \d+ (.+)'
  
  projects = re.findall(pattern, text)

 
  

  final_output = []

  for project in projects:
      final_output.append("".join(project))
        
  for i in range(len(final_output)):
      final_output[i] = re.sub('  ', ' ', final_output[i])
      final_output[i] = final_output[i].strip()
      final_output[i] += '...'


  if len(final_output)==0:
    s=f"{find_client_name(text)} corporation has hired our team for {find_project_name(text,path)} to improve and increase their business.The project started from {find_start_date(text)} and expected to be completed by {find_end_date(text)} the budget for the project was {find_budget(text)}."
    return s


  return final_output

# def find_url(text):
    
#     pattern = r"(https?://\S+)"
    
#     urls = re.findall(pattern, text) 
            
#     return urls

from zmq import NULL

from googlesearch import search
import sys

def googleSearch(keyword):
  results = search(keyword)
  # count = 0
  print(results)  
  for search_result in results:
    return search_result

def find_url(text,path):
    
    pattern = r"(https?://\S+)"
    
    urls = re.findall(pattern, text)
    if urls is None:
      curr_url=googleSearch(find_project_name(text,path))
      return curr_url
    else:
      return urls

# This paste expires in <8 hours. Public IP access. Share whatever you see with others in seconds with Context.Terms of ServiceReport this

def find_location(text):
    geo = []
    doc = nlp(text)
    # pattern
    pattern = [{'LOWER' : 'geography'}, {}]
    
    
    # Matcher class object 
    matcher = Matcher(nlp.vocab)
    matcher.add("geo", [pattern])
    
    matches = matcher(doc)
    
#     print(matches)

    # finding patterns in the text
    for i in range(0,len(matches)):
        
        # match: id, start, end
        token = doc[matches[i][1]+1:matches[i][2]]
        # append token to list
        geo.append(str(token))

    return geo

def find_start_date(text):
    
    dates = []
    doc = nlp(text)
    # pattern
    pattern = [{'lower' : 'start'}, {'LOWER' : 'date'}, {}]
    
    
    # Matcher class object 
    matcher = Matcher(nlp.vocab)
    matcher.add("startdate", [pattern])
    
    matches = matcher(doc)
    
#     print(matches)

    # finding patterns in the text
    for i in range(0,len(matches)):
        
        # match: id, start, end
        token = doc[matches[i][1]+2:matches[i][2]]
        # append token to list
        dates.append(str(token))

    return dates

def find_end_date(text):
    
    dates = []
    doc = nlp(text)
    # pattern
    pattern = [{'lower' : 'end'}, {'LOWER' : 'date'}, {}]
    
    
    # Matcher class object 
    matcher = Matcher(nlp.vocab)
    matcher.add("startdate", [pattern])
    
    matches = matcher(doc)
    
#     print(matches)

    # finding patterns in the text
    for i in range(0,len(matches)):
        
        # match: id, start, end
        token = doc[matches[i][1]+2:matches[i][2]]
        # append token to list
        dates.append(str(token))

    return dates

def find_budget(text):
    budgets = []

    doc = nlp(text)
    # pattern
    pattern = [{'lower' : 'budget'}, {'LOWER' : 'as'}, {'LOWER' : 'per'}, {'LOWER' : 'sow'}, {'IS_CURRENCY' : True, 'OP' : '?'}, {}]
    
    
    # Matcher class object 
    matcher = Matcher(nlp.vocab)
    matcher.add("budgets", [pattern])
    
    matches = matcher(doc)
    
#     print(matches)

    # finding patterns in the text
    for i in range(0,len(matches)):
        
        # match: id, start, end
        token = doc[matches[i][1]+4:matches[i][2]]
        
        token = re.sub(',', '', str(token))
        # append token to list
        budgets.append(str(token))

    return budgets

def find_vertical(text):
  verticals = []
  doc = nlp(text)
  # pattern
  pattern = [{'LOWER' : 'vertical'}, {'IS_PUNCT' : True}, {}]
  
  
  # Matcher class object 
  matcher = Matcher(nlp.vocab)
  matcher.add("vertical", [pattern])
  
  matches = matcher(doc)
  
#     print(matches)

  # finding patterns in the text
  for i in range(0,len(matches)):
      
      # match: id, start, end
      token = doc[matches[i][1]+2:matches[i][2]]
      # append token to list
      verticals.append(str(token))

  return verticals

def find_techstack(text):
    
  bow = ["Oracle", "business intelligence","ERP","salesforce", "CRM", "Lawson DB", "cfg","SC management", "demand management","DA", 
         "market intelligence","computer generated solutions", "cloud", "development","SSIS","SSRS","SSAS", "analysis", "tracking", "OSC",
         "BI","AWS","ETL", "data integration","reporting", "insights", "charts","BI", "Microsoft", "ETL", "SSIS","Reporting","open source", 
         "storage", "clustering","HRM", "CRM", "EPM", "FSCM","retrieval", "transformation", "services", "budgeting", "forcasting","Amazon", 
         "storage", "Elastic Block storage","storage","fusion","testing", "test automation", "BI Applications", "Database", "data integration",
         "SSO", "Authentication","object oriented", "language","Linux", "OS","reporting", "SAP crystal", "lumira","SAP", "crystal syntax",
         "reporting", "analysis","fusion", "OBI","ETL Tool", "Data Integration","Data Visualization", "Reporting","Data Analytics",
         "Data Warehouse","Storage", "Cloud Database","Business Intelligence", "Enterprise Reporting","Business Analytics", "Visualization",
         "Data Interchange","Web Connection", "Integration", "API Management","Cloud Solutions", "Data Patterns","Claim Management", 
         "Insurance","ETL Tool","Data Integration", "Server Database","Data Virtualization", "ETL Tool", "Data Management","Cloud Services",
         "Business Intelligence","Data Analysis", "Database Management","Data Warehouse", "Data Analytics","Interpreter",
         "Scripting Language","AWS", "Storage", "Bucket","Data Quality","BI Cloud Solution","AI Framework","ETL", "Integration", 
         "Data Ingestion","Business Intelligence", "Reporting","Data Manipulation", "Batch Processing","Integration Tool", "ETL",
         "Cloud Software","ERP","Terminal/Console", "SSH Client", "File Transfer","Relational Database", "Cloud Storage","Big Data", 
         "Data Lake", "Cloud Service"]


  bow = [x.lower() for x in bow]

  techstack = set()

  for ele in bow:
    if ele in text:
      techstack.add(ele)

  return techstack

def find_tools(text):

  bow = ["DA","Reports", "Dashboards","Snowflake", "ETL", "Oracle"]


  bow = [x.lower() for x in bow]

  tools = set()

  for ele in bow:
    if ele in text:
      tools.add(ele)

  return tools

def find_accelerators(text):
  
  bow = ["report validation", "performance tuning","DW validation", "initial load","Data Warehouse", "Data Analytics", "Data Insights"]
  bow = [x.lower() for x in bow]

  accelerators = set()

  for ele in bow:
    if ele in text:
      accelerators.add(ele)

  return accelerators

def find_team(text):
    
    team_members = []

    
    if len(text.split('Team')) == 1:
        return []

    final_text = text.split('Team')[1]

    final_text = final_text.split(':')[0]

    if(final_text):

      doc = nlp(final_text)

      for entity in doc.ents:

          if entity.label_ == 'PERSON':
              team_members.append(entity.text)

    
    return(team_members)

features = {
      # 'project_name' : [],
      # 'client_name' : [],
      # 'project_desc':[],
      # 'url':[],
      # 'location':[],
      # 'start_date':[],
      # 'end_date':[],
      # 'budget':[],
      # 'vertical':[],
      # 'techstack':[],
      # 'tools':[],
      # 'accelerators':[],
      # 'team':[]
  }

# driver('/content/InputData/Activision Case Study.pdf')
def driver(path):


  

  text = read_pdf(path)
  text = clean(text)
  # dict['gg']=text

  # print(text)

  features['project_name'] = find_project_name(text,path.split('/')[-1])
  features['client_name'] = find_client_name(path.split('/')[-1])
  features['project_desc'] = find_project_desc(text,path.split('/')[-1])
  features['url'] = find_url(text,path.split('/')[-1])
  features['location'] = find_location(text)
  features['start_date'] = find_start_date(text)
  features['end_date'] = find_end_date(text)
  features['budget'] = find_budget(text)
  features['vertical'] = find_vertical(text)
  features['techstack'] = find_techstack(text)
  features['tools'] = find_tools(text)
  features['accelerators'] = find_accelerators(text)
  features['team'] = find_team(text)

  # FinalOutput = pd.DataFrame.from_dict(features,orient='index') 
  # result=FinalOutput.transpose()
  # result = result.replace([None], [''], regex=True)
  # print(features)
  
  return features

# FinalOutput = pd.DataFrame.from_dict(features,orient='index') 
# result=FinalOutput.transpose()

# FinalOutput

folder_path = '/content/data'
file_list = os.listdir(folder_path)
for index,pdf_file in enumerate(file_list):
  print(index,driver(os.path.join(folder_path, pdf_file)))

for pdf_file in file_list:
  print(os.path.join(folder_path, pdf_file))

# ##########################3

# FOR DATAFRAME

df['clean_data'] = df['file_data'].apply(clean)



for pdf_file in file_list:
  gp=os.path.join(folder_path, pdf_file)
  df['project_name'] = df['clean_data'].apply(lambda x:find_project_name(x,gp.split('/')[-1]))
  df['Client Name'] = df['file_name'].apply(find_client_name)
  df['start_date'] = df['clean_data'].apply(find_start_date)
  df['end_date'] = df['clean_data'].apply(find_end_date)
  df['url'] = df['clean_data'].apply(lambda x:find_url(x,gp.split('/')[-1]))
  df['Project Description'] = df['clean_data'].apply(lambda x:find_project_desc(x,gp.split('/')[-1]))
  df['team'] = df['clean_data'].apply(find_team)
  df['budget'] = df['clean_data'].apply(find_budget)
  df['vertical'] = df['clean_data'].apply(find_vertical)
  df['Geography'] = df['clean_data'].apply(find_location)
  df['Techstack'] = df['clean_data'].apply(find_techstack)
  df['Tools'] = df['clean_data'].apply(find_tools)
  df['Accelerators'] = df['clean_data'].apply(find_accelerators)



df=df.drop(['file_name','file_path','file_extension','file_data',],axis=1)





























